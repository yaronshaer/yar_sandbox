{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Jupyter notebook!\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello Jupyter notebook!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import for loading env variables\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "#now let's import the ai stuff\n",
    "import openai\n",
    "import langchain\n",
    "#import vectorestore we will be using\n",
    "from langchain.vectorstores import FAISS\n",
    "#load the easy loader for text\n",
    "from langchain.document_loaders import TextLoader\n",
    "#load wikipedia loader\n",
    "from langchain.document_loaders import WikipediaLoader\n",
    "#now import the embedding engine\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's load a wikipedia document. \n",
    "### Langchain has standard capabilities to load many types of data to its loader. see link **[here](https://python.langchain.com/docs/modules/data_connection/document_loaders)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load content from Wikipedia using WikipediaLoader\n",
    "loader = WikipediaLoader(\"Klarna\",doc_content_chars_max=100000, load_max_docs=1)\n",
    "doc = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you have loaded 1 documents\n",
      "the document contains 8682 characters\n",
      "here are the first 100 characters: Klarna Bank AB, commonly referred to as Klarna, is a Swedish fintech company that provides online fi\n"
     ]
    }
   ],
   "source": [
    "print(f\"you have loaded {len(doc)} documents\")\n",
    "print(f\"the document contains {len(doc[0].page_content)} characters\")\n",
    "print(f\"here are the first 100 characters: {doc[0].page_content[0:100]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need to split our wikipedia doc into smaller chunks. we do this with text splitter, there are several to choose from offered by langchain. Check **[this](https://python.langchain.com/docs/modules/data_connection/document_transformers/)** out for langchain's document loader built-in integrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=400)\n",
    "docs = splitter.split_documents(doc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check that it worked and we have multiple documents from the single wikipedia one we loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nowe have 7 documents with an average 1,296 chars per document\n"
     ]
    }
   ],
   "source": [
    "total_chars_in_docs = sum([len(x.page_content) for x in docs])\n",
    "print(f\"nowe have {len(docs)} documents with an average {total_chars_in_docs / len(docs):,.0f} chars per document\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Create embeddings and store in **[FAISS](https://python.langchain.com/docs/modules/data_connection/vectorstores/integrations/faiss)** vectorestore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.23it/s]\n"
     ]
    }
   ],
   "source": [
    "#Get embeddings engine ready\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "#embed the documents and combine with raw text in a pseudo vectorstore. This will make a call to openai \n",
    "docsearch = FAISS.from_documents(docs, embeddings)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before we can ask questions about our document, we need to use a retriver. \n",
    "Read about retrivers  **[here](https://python.langchain.com/docs/modules/data_connection/retrievers/)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
