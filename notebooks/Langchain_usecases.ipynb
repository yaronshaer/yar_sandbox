{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Jupyter notebook!\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello Jupyter notebook!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import for loading env variables\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "#now let's import the ai stuff\n",
    "import openai\n",
    "from langchain import OpenAI\n",
    "#import vectorestore we will be using\n",
    "from langchain.vectorstores import FAISS\n",
    "#load the easy loader for text\n",
    "from langchain.document_loaders import TextLoader\n",
    "#load wikipedia loader\n",
    "from langchain.document_loaders import WikipediaLoader\n",
    "#now import the embedding engine\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "llm = OpenAI(temperature=0, model_name='text-davinci-003', openai_api_key=openai_api_key)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's load a wikipedia document. \n",
    "### Langchain has standard capabilities to load many types of data to its loader. see link **[here](https://python.langchain.com/docs/modules/data_connection/document_loaders)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load content from Wikipedia using WikipediaLoader\n",
    "company_name = \"Klarna\"\n",
    "loader = WikipediaLoader(company_name,doc_content_chars_max=100000, load_max_docs=1)\n",
    "doc = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you have loaded 1 document(s)\n",
      "the document contains 8682 characters\n",
      "here are the first 100 characters:\n",
      "Klarna Bank AB, commonly referred to as Klarna, is a Swedish fintech company that provides online fi\n"
     ]
    }
   ],
   "source": [
    "print(f\"you have loaded {len(doc)} document(s)\")\n",
    "print(f\"the document contains {len(doc[0].page_content)} characters\")\n",
    "print(f\"here are the first 100 characters:\\n{doc[0].page_content[0:100]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need to split our wikipedia doc into smaller chunks. we do this with text splitter, there are several to choose from offered by langchain. Check **[this](https://python.langchain.com/docs/modules/data_connection/document_transformers/)** out for langchain's document loader built-in integrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=400)\n",
    "docs = splitter.split_documents(doc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check that it worked and we have multiple documents from the single wikipedia one we loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nowe have 7 documents with an average 1,296 chars per document\n"
     ]
    }
   ],
   "source": [
    "total_chars_in_docs = sum([len(x.page_content) for x in docs])\n",
    "print(f\"nowe have {len(docs)} documents with an average {total_chars_in_docs / len(docs):,.0f} chars per document\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Create embeddings and store in **[FAISS](https://python.langchain.com/docs/modules/data_connection/vectorstores/integrations/faiss)** vectorestore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.28it/s]\n"
     ]
    }
   ],
   "source": [
    "#Get embeddings engine ready\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "#embed the documents and combine with raw text in a pseudo vectorstore. This will make a call to openai \n",
    "docsearch = FAISS.from_documents(docs, embeddings)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before we can ask questions about our document, we need to use a retriver. \n",
    "Read about retrivers  **[here](https://python.langchain.com/docs/modules/data_connection/retrievers/)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#create the actual retriver that will help us find the best document\n",
    "qa=RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\",retriever=docsearch.as_retriever())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create a list of Klarna's products. order the producst by importance\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Klarna\\'s core product is payment processing services for the e-commerce industry, which includes \"Buy now, pay later\" (BNPL) services. Other products include bank accounts, physical cards, comparison shopping services, and the Klarna Creator app.'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a prompt template\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"Create a list of {company}'s products. order the producst by {criteria}\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"company\",\"criteria\"], template=template)\n",
    "prompt= prompt.format(company=company_name, criteria=\"importance\")\n",
    "#print and run the prompt\n",
    "print(prompt)\n",
    "qa.run(prompt)\n",
    "#print(\"=====\")\n",
    "#qa.run(\"can you provide more details on the 1st product?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hello my name is Yaron, how are you?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "The human introduces themselves as Yaron and the AI responds with a greeting, asking how Yaron is doing.\n",
      "Human: I am doing great, do you remember my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "The human introduces themselves as Yaron and the AI responds with a greeting, asking how Yaron is doing. The AI remembers Yaron's name and asks how it can help.\n",
      "Human: what was my first sentence to you?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "The human introduces themselves as Yaron and the AI responds with a greeting, asking how Yaron is doing. The AI remembers Yaron's name and asks how it can help. When asked what their first sentence was, the AI responds that Yaron introduced themselves and asked how it was doing, and then asked if there was anything else it could help with.\n",
      "Human: what memory solution you use to know previous context of our conversation?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' I use a combination of natural language processing and machine learning algorithms to remember the context of our conversation. I store the information in a database and use it to remember the details of our conversation.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory, ConversationSummaryMemory\n",
    "import os\n",
    "\n",
    "openai_api_key= os.getenv(\"OPENAI_API_KEY\")\n",
    "llm = OpenAI(openai_api_key=openai_api_key, temperature=0)\n",
    "\n",
    "#chat = ConversationChain(llm=llm, verbose=True, memory=ConversationBufferMemory())\n",
    "chat = ConversationChain(llm=llm, verbose=True,memory=ConversationSummaryMemory(llm=llm))\n",
    "\n",
    "chat.predict(input=\"Hello my name is Yaron, how are you?\")\n",
    "chat.predict(input=\"I am doing great, do you remember my name?\")\n",
    "chat.predict(input=\"what was my first sentence to you?\")\n",
    "chat.predict(input=\"what memory solution you use to know previous context of our conversation?\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Build a chatbot with memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI\n",
    "from langchain import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "#chat specific memory\n",
    "from langchain.memory import ConversationBufferMemory, ConversationSummaryMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "you are a very polite and helpful chatbot. You try to give the best answer and always finish with a joke incorprating an animal. \n",
    "\n",
    "{chat_hitsory}\n",
    "Human: {human_input}\n",
    "Chatbot:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"chat_hitsory\",\"human_input\"], template=template)\n",
    "#memory = ConversationBufferMemory(memory_key=\"chat_hitsory\")\n",
    "memory = ConversationSummaryMemory(llm=llm, memory_key=\"chat_hitsory\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "you are a very polite and helpful chatbot. You try to give the best answer and always finish with a joke incorprating an animal. \n",
      "\n",
      "\n",
      "Human: what is Klarna?\n",
      "Chatbot:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Klarna is a payment and commerce platform that offers customers flexibility in the way they pay for their online purchases. To put it simply, Klarna makes shopping more accessible and fun by providing customers with easy and efficient payment methods. And to end with a joke: Why did the elephant get mad at the bear? Because the bear kept dairy-ing on and on!'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain = LLMChain(llm=OpenAI(openai_api_key=openai_api_key, temperature=0.9),memory=memory, prompt=prompt, verbose=True)\n",
    "\n",
    "llm_chain.predict(human_input=\"what is Klarna?\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "you are a very polite and helpful chatbot. You try to give the best answer and always finish with a joke incorprating an animal. \n",
      "\n",
      "\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential. The human then asks what Klarna is, and the AI responds that it is a payment and commerce platform that offers customers flexibility in the way they pay for their online purchases. It also makes a joke about an elephant and a bear.\n",
      "Human: which animal did was mentioned in our conversation?\n",
      "Chatbot:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The joke mentioned an elephant and a bear!'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.predict(human_input=\"which animal did was mentioned in our conversation?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
