{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import for loading env variables\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's load a wikipedia document. \n",
    "### Langchain has standard capabilities to load many types of data to its loader. see link **[here](https://python.langchain.com/docs/modules/data_connection/document_loaders)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now let's import the ai stuff\n",
    "import openai\n",
    "from langchain import OpenAI\n",
    "#import vectorestore we will be using\n",
    "from langchain.vectorstores import FAISS\n",
    "#load the easy loader for text\n",
    "from langchain.document_loaders import TextLoader\n",
    "#load wikipedia loader\n",
    "from langchain.document_loaders import WikipediaLoader\n",
    "#now import the embedding engine\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.5, model_name='text-davinci-003', openai_api_key=openai_api_key)\n",
    "\n",
    "# Load content from Wikipedia using WikipediaLoader\n",
    "company_name = \"Klarna\"\n",
    "loader = WikipediaLoader(company_name,doc_content_chars_max=100000, load_max_docs=1)\n",
    "doc = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"you have loaded {len(doc)} document(s)\")\n",
    "print(f\"the document contains {len(doc[0].page_content)} characters\")\n",
    "print(f\"here are the first 100 characters:\\n{doc[0].page_content[0:100]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need to split our wikipedia doc into smaller chunks. we do this with text splitter, there are several to choose from offered by langchain. Check **[this](https://python.langchain.com/docs/modules/data_connection/document_transformers/)** out for langchain's document loader built-in integrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=400)\n",
    "docs = splitter.split_documents(doc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check that it worked and we have multiple documents from the single wikipedia one we loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nowe have 7 documents with an average 1,296 chars per document\n"
     ]
    }
   ],
   "source": [
    "total_chars_in_docs = sum([len(x.page_content) for x in docs])\n",
    "print(f\"nowe have {len(docs)} documents with an average {total_chars_in_docs / len(docs):,.0f} chars per document\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Create embeddings and store in **[FAISS](https://python.langchain.com/docs/modules/data_connection/vectorstores/integrations/faiss)** vectorestore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get embeddings engine ready\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "#embed the documents and combine with raw text in a pseudo vectorstore. This will make a call to openai \n",
    "docsearch = FAISS.from_documents(docs, embeddings)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before we can ask questions about our document, we need to use a retriver. \n",
    "Read about retrivers  **[here](https://python.langchain.com/docs/modules/data_connection/retrievers/)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#create the actual retriver that will help us find the best document\n",
    "qa=RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\",retriever=docsearch.as_retriever())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a prompt template\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"Create a numbered list of {company}'s products. order the producst by {criteria}. please name the products in the list.\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"company\",\"criteria\"], template=template)\n",
    "prompt= prompt.format(company=company_name, criteria=\"lanunch date\")\n",
    "#print and run the prompt\n",
    "print(prompt)\n",
    "qa.run(prompt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa.run(\"what was the previosu question I asked you? please be percise and use the same words I used\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a simple chatbot with memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory, ConversationSummaryMemory\n",
    "import os\n",
    "\n",
    "openai_api_key= os.getenv(\"OPENAI_API_KEY\")\n",
    "llm = OpenAI(openai_api_key=openai_api_key, temperature=0.4)\n",
    "\n",
    "#chat = ConversationChain(llm=llm, verbose=True, memory=ConversationBufferMemory())\n",
    "chat = ConversationChain(llm=llm, verbose=True,memory=ConversationSummaryMemory(llm=llm))\n",
    "\n",
    "chat.predict(input=\"Hello my name is Yaron, how are you?\")\n",
    "chat.predict(input=\"I am doing great, do you remember my name?\")\n",
    "chat.predict(input=\"what was my first sentence to you?\")\n",
    "chat.predict(input=\"what memory solution you use to know previous context of our conversation?\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Build a chatbot with memory and include prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI\n",
    "from langchain import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "#chat specific memory\n",
    "from langchain.memory import ConversationBufferMemory, ConversationSummaryMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "you are a very polite and helpful chatbot. You try to give the best answer and always finish with a joke incorprating an animal. \n",
    "\n",
    "{chat_hitsory}\n",
    "Human: {human_input}\n",
    "Chatbot:\n",
    "\"\"\"\n",
    "\n",
    "#create the prmpt from the template\n",
    "prompt = PromptTemplate(input_variables=[\"chat_hitsory\",\"human_input\"], template=template)\n",
    "\n",
    "#define the memory for the chatbot\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_hitsory\")\n",
    "#memory = ConversationSummaryMemory(llm=llm, memory_key=\"chat_hitsory\")\n",
    "#create the llm \n",
    "llm = OpenAI(openai_api_key=openai_api_key, temperature=0.4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(llm=llm,memory=memory, prompt=prompt, verbose=True)\n",
    "\n",
    "\n",
    "llm_chain.predict(human_input=\"what is Klarna?\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain.predict(human_input=\"which animal did was mentioned in our conversation?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents\n",
    "\n",
    "Read the *[LangChain Agent Docs](https://python.langchain.com/en/latest/modules/agents.html)*\n",
    "\n",
    "Agents are the decision makers that can look a data, reason about what the next action should be, and execute that action for you via tools. Langchain supports integrations to a variety of tools. Read more about the supported tools *[here](https://python.langchain.com/docs/modules/agents/tools/)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from langchain.llms import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "#agents related imports\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "\n",
    "#now import tools\n",
    "from langchain.agents import Tool\n",
    "from langchain.utilities import GoogleSearchAPIWrapper\n",
    "from langchain.utilities import TextRequestsWrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "google_cse_id = os.getenv(\"GOOGLE_CSE_ID\")\n",
    "\n",
    "llm = OpenAI(openai_api_key=openai_api_key, temperature=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise the tools the agent will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = GoogleSearchAPIWrapper()\n",
    "requests = TextRequestsWrapper()\n",
    "\n",
    "#put tools in toolkit lits\n",
    "toolkit = [\n",
    "    Tool(name=\"Search\", func=search.run, description=\"useful when you want to seatch web for current information\"),\n",
    "    Tool(name=\"Requests\", func=requests.get, description=\"useful when you need to make a request to a website or url\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent initialization\n",
    "agent = initialize_agent(tools=toolkit, llm=llm, verbose=True, agent=\"zero-shot-react-description\", return_intermediate_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ask the agent a question\n",
    "#response = agent({\"input\":\"what is the capital of israel?\"})\n",
    "response = agent(\"{what are famour fintechs founded in Sweden?}\")\n",
    "response['output']\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
